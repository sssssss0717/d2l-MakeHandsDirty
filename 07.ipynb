{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31167e2-3868-4345-8b85-88aafce0160a",
   "metadata": {},
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9373a-dbf7-490e-b2e3-1b010865a941",
   "metadata": {},
   "source": [
    "**自动求导（autodiff）中的两种主要模式** \n",
    "—— 正向累积（forward accumulation）和反向累积（reverse accumulation，也叫反向传播），\n",
    "这是现代深度学习框架（如 PyTorch、TensorFlow）实现梯度计算的核心原理之一。\n",
    "\n",
    "下面我们**按层级**详细讲解：\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 1. 链式法则（Chain Rule）\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u_n} \\cdot \\frac{\\partial u_n}{\\partial u_{n-1}} \\cdot \\dots \\cdot \\frac{\\partial u_2}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x}\n",
    "$$\n",
    "\n",
    "这是最基本的**链式求导规则**，适用于变量经过多层函数嵌套的情形：\n",
    "\n",
    "例如：\n",
    "- $x \\rightarrow u_1 = f_1(x)$\n",
    "- $u_1 \\rightarrow u_2 = f_2(u_1)$\n",
    "- ...\n",
    "- $u_n \\rightarrow y = f_{n+1}(u_n)$\n",
    "\n",
    "那么：\n",
    "- $y = f_{n+1} \\circ f_n \\circ \\dots \\circ f_1(x)$\n",
    "- 对 $x$ 求导时，就必须逐层相乘所有的偏导数\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 2. 正向累积（Forward Mode）\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} =\n",
    "\\frac{\\partial y}{\\partial u_n} \\left( \\frac{\\partial u_n}{\\partial u_{n-1}} \\left( \\dots \\left( \\frac{\\partial u_2}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x} \\right) \\dots \\right) \\right)\n",
    "$$\n",
    "\n",
    "### ✅ 理解方式：\n",
    "\n",
    "- **从输入开始**（从 $x$）一步一步正向地传播导数\n",
    "- 每一步将当前的导数乘上下一级的偏导\n",
    "- 适合：**输入变量少，输出变量多** 的情形（例如：标量输入 → 向量输出）\n",
    "\n",
    "### 💡 应用场景：\n",
    "- 科学计算中的灵敏度分析\n",
    "- 编译器层的自动微分（如 JAX）\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 3. 反向累积（Reverse Mode，也称反向传播）\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} =\n",
    "\\left( \\left( \\left( \\left( \\frac{\\partial y}{\\partial u_n} \\cdot \\frac{\\partial u_n}{\\partial u_{n-1}} \\right) \\cdot \\dots \\right) \\cdot \\frac{\\partial u_2}{\\partial u_1} \\right) \\cdot \\frac{\\partial u_1}{\\partial x} \\right)\n",
    "$$\n",
    "\n",
    "### ✅ 理解方式：\n",
    "\n",
    "- 从**输出开始**（从 $y$）向后一步一步反向传播梯度\n",
    "- 每一步把当前的梯度乘以上一级变量的偏导\n",
    "- 适合：**输入变量多，输出变量少（尤其是标量）** 的情形\n",
    "\n",
    "### 💡 这正是深度学习框架中最常用的“**反向传播（Backpropagation）**”原理！\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 比较总结\n",
    "\n",
    "| 特点 | 正向模式 | 反向模式（反向传播） |\n",
    "|------|----------|------------------------|\n",
    "| 方向 | 从输入向输出 | 从输出向输入 |\n",
    "| 优点 | 适合少输入多输出 | 适合多输入单输出（典型深度学习） |\n",
    "| 应用 | JAX / 数值模拟 | PyTorch / TensorFlow 等神经网络框架 |\n",
    "| 难点 | 存储导数信息 | 需要构建计算图和缓存中间变量 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 小贴士：\n",
    "\n",
    "- PyTorch 默认采用 **反向传播模式**，执行 `loss.backward()` 时就是执行了这张图中最下面那一行公式。\n",
    "- JAX 可以使用正向或反向模式自动微分，适合科研工程。\n",
    "- 你现在看到的推导其实就是计算图上使用链式法则的不同实现方式。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结一句话：\n",
    "\n",
    "> 自动求导的本质就是链式法则，但方向不同（正向 or 反向），适合的应用场景也不同。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b0220-e794-4787-8699-1f7d2e9be865",
   "metadata": {},
   "source": [
    "## 自动求导的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9e86d-1b62-4e34-9bc0-3a1bc8872bf2",
   "metadata": {},
   "source": [
    "假设我们想对函数$y = 2X^TX$关于列向量$X$求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb59242-7d40-4318-9478-345e1b6c53ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b30431-0036-4857-9f2a-5c890bd19148",
   "metadata": {},
   "source": [
    "在我们计算$y$关于$X$的梯度之前，我们需要一个地方来存储梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afccc7b9-9371-433a-bd0a-0ffa6925f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  #等价于'x = torch.arange(4.0, requires_grad = True)'\n",
    "x.grad  #默认时None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24b1e13-a8c5-4188-bded-4d56a20ce2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb849e-ff2d-481a-8c60-af3387d540c2",
   "metadata": {},
   "source": [
    "通过用反向传播函数来自动计算$y$关于$X$每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fa0e2f-7130-43b7-b59d-1b241d653e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3a599b-f8ad-4442-8891-33a2da3ba85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a08247-d8a2-4e36-bdd8-c1e6d3bb8cf3",
   "metadata": {},
   "source": [
    "这段代码是 PyTorch 中 **自动求导（autograd）机制的一个典型例子**，用来演示如何对一个**标量函数**进行 **反向传播（backward）**，并计算输入变量的梯度。\n",
    "\n",
    "我们来**逐行讲解每一个细节**👇\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 代码回顾：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)  # tensor([0., 1., 2., 3.])\n",
    "x.requires_grad_(True)  # 开启梯度跟踪\n",
    "\n",
    "x.grad  # 此时为 None，因为还没进行反向传播\n",
    "\n",
    "y = 2 * torch.dot(x, x)  # y 是一个标量（内积结果 * 2）\n",
    "y.backward()  # 反向传播，计算 dy/dx\n",
    "x.grad  # 查看梯度\n",
    "x.grad == 4 * x  # 是否等于 4x？（验证梯度结果）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 第一步：`x = torch.arange(4.0)`\n",
    "\n",
    "```python\n",
    "x = tensor([0., 1., 2., 3.])\n",
    "```\n",
    "\n",
    "创建一个一维张量 $\\mathbf{x} = [0, 1, 2, 3]$，元素类型为 float。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 第二步：`x.requires_grad_(True)`\n",
    "\n",
    "开启 **梯度跟踪**，告诉 PyTorch：\n",
    "\n",
    "> “以后在计算图中涉及 `x` 的运算，都要记录下来，以便之后可以反向传播求梯度。”\n",
    "\n",
    "这时 `x.grad` 还为 `None`，因为我们**还没有执行 `backward()`**。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 第三步：`y = 2 * torch.dot(x, x)`\n",
    "\n",
    "计算内积再乘 2：\n",
    "\n",
    "$$\n",
    "y = 2 \\cdot (\\mathbf{x}^\\top \\mathbf{x}) = 2 \\cdot \\sum_{i=0}^{3} x_i^2\n",
    "$$\n",
    "\n",
    "用公式写是：\n",
    "\n",
    "$$\n",
    "y = 2(x_0^2 + x_1^2 + x_2^2 + x_3^2)\n",
    "$$\n",
    "\n",
    "代入实际数值：\n",
    "$$\n",
    "y = 2 \\cdot (0^2 + 1^2 + 2^2 + 3^2) = 2 \\cdot (0 + 1 + 4 + 9) = 2 \\cdot 14 = 28\n",
    "$$\n",
    "\n",
    "注意：**y 是一个标量**，所以可以直接对它调用 `backward()`。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 第四步：`y.backward()`\n",
    "\n",
    "这是自动求导的核心操作，它会从标量 `y` 出发，在记录好的计算图中反向传播，计算所有 `requires_grad=True` 张量的梯度。\n",
    "\n",
    "也就是计算：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( 2 \\sum x_i^2 \\right) = 2 \\cdot 2x_i = 4x_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 第五步：`x.grad`\n",
    "\n",
    "现在你可以查看 `x` 的梯度了：\n",
    "\n",
    "```python\n",
    "x.grad  →  tensor([ 0.,  4.,  8., 12.])\n",
    "```\n",
    "\n",
    "这正是 $4x$ 的结果！\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 第六步：`x.grad == 4 * x`\n",
    "\n",
    "这是一个**逻辑验证**，我们手动计算 $4x$，然后看是否和自动求导得到的 `x.grad` 一致。\n",
    "\n",
    "```python\n",
    "4 * x → tensor([ 0.,  4.,  8., 12.])\n",
    "```\n",
    "\n",
    "所以：\n",
    "\n",
    "```python\n",
    "x.grad == 4 * x  → tensor([True, True, True, True])\n",
    "```\n",
    "\n",
    "全部为 `True`，说明 **PyTorch 的反向传播计算完全正确！**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结小抄：\n",
    "\n",
    "| 步骤 | 含义 |\n",
    "|------|------|\n",
    "| `x.requires_grad_(True)` | 开启梯度追踪 |\n",
    "| `y = f(x)` | 构造计算图 |\n",
    "| `y.backward()` | 自动求导（反向传播） |\n",
    "| `x.grad` | 存储 $\\frac{\\partial y}{\\partial x}$ 的结果 |\n",
    "| 验证 | 和手动计算结果对比 |\n",
    "\n",
    "---\n",
    "\n",
    "如果你想更深入了解计算图是怎么构建的，或者我可以用图示画出这段计算的结构，也可以用 PyTorch 的 `grad` 函数手动求导来辅助理解，要试试看吗？😄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d935529-d3db-4eb5-9f81-5f4a2103c47b",
   "metadata": {},
   "source": [
    "现在我们计算$X$的另外一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522880e3-33a9-4e04-b671-f3e91469bb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，pytorch会积累梯度，我们需要清楚之前的值。\n",
    "x.grad.zero_()  # '_' 表示原地操作\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8963e-d3d5-47a8-a08e-fd1890349ea1",
   "metadata": {},
   "source": [
    "深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c774983d-4864-4067-9975-94637cb2e6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用'backward'需要传入一个'gradient'参数，该参数指定微分函数\n",
    "x.grad.zero_()  \n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f1718-4098-4d7d-b5d6-b26a691cd4dc",
   "metadata": {},
   "source": [
    "将某些计算移动到记录的计算图之外。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ca93fc-fd75-4705-9d15-e51a3578e57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b34bde-b256-4e7f-be0e-7cc37c409088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac915259-2826-47f2-a103-e727c236ee76",
   "metadata": {},
   "source": [
    "即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们任然可以计算得到变量的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97027f12-d7d7-46de-ae56-231ce519aa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size = (), requires_grad = True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd517e07-732e-46ee-a813-0a61be92fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "标量，向量？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38301219-cf1c-492a-8dc4-7b48f76bf4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6691ce-8d43-43e3-9bc9-432d8802d824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32d5aa-3094-4aeb-a6b4-ba7659eb4248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff02787-ef23-4aef-bf10-7831c9102725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1d47b-0cc2-497f-bd43-6fd2630a95a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d766b-b7db-42dd-864d-0a527b6374e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1960079-ad4e-4113-8141-f36bd17ca899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a081e3-b2d4-473b-b918-806b041b7f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4edb8-384c-4e71-beb3-7bc301e2f155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d0db5-4806-405e-8012-86802e08edac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
